# 엘라스틱서치를 위한 Semi-Structured 텍스트 분석 포맷 설계

최근 로그 데이터를 분석하기 위한 semi-structured 텍스트 포맷을 설계해드리겠습니다. 로그 데이터의 특성을 고려하여 유연하면서도 구조화된 포맷을 제안합니다.

## 기본 포맷 구조

```
[타임스탬프] [로그레벨] [서비스/모듈명] [트랜잭션ID] [호스트/IP] 메시지 내용 [키=값 키=값 ...]
```

## 상세 필드 설명

1. **타임스탬프**: ISO 8601 형식 권장
   - 예: `2023-11-15T14:32:45.123Z`

2. **로그레벨**: 대문자로 표기 (ERROR, WARN, INFO, DEBUG, TRACE)
   - 예: `INFO`

3. **서비스/모듈명**: 로그를 발생시킨 서비스 또는 모듈 이름
   - 예: `order-service`, `auth-module`

4. **트랜잭션ID**: 요청을 추적하기 위한 고유 ID (UUID 권장)
   - 예: `txid-8a7f6e5d4c3b2a1`

5. **호스트/IP**: 로그를 발생시킨 서버 정보
   - 예: `host-web-01`, `10.0.0.1`

6. **메시지 내용**: 로그의 주요 내용 (자유 텍스트)

7. **키-값 쌍**: 추가 메타데이터 (공백으로 구분, 값에 공백이 있을 경우 따옴표로 감쌈)
   - 예: `user_id=12345 action="create order" duration_ms=245`

## 실제 예시

```
2023-11-15T14:32:45.123Z INFO order-service txid-8a7f6e5d4c3b2a1 host-web-01 Order created successfully order_id=789 user_id=12345 items=3 total_amount=45000 payment_method=credit_card
```

```
2023-11-15T14:33:01.456Z ERROR payment-service txid-9b8c7d6e5f4a3b2 host-pay-02 Payment processing failed error_code=PAYMENT_DECLINED user_id=12345 amount=45000 reason="Insufficient funds" retry_count=2
```

## 엘라스틱서치 인덱스 매핑 예제

이 로그 포맷을 분석하기 위한 엘라스틱서치 인덱스 매핑 예제:

```json
{
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date",
        "format": "strict_date_optional_time||epoch_millis"
      },
      "log_level": {
        "type": "keyword"
      },
      "service": {
        "type": "keyword"
      },
      "transaction_id": {
        "type": "keyword"
      },
      "host": {
        "type": "keyword"
      },
      "message": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 256
          }
        }
      },
      "metadata": {
        "type": "nested",
        "properties": {
          "key": {
            "type": "keyword"
          },
          "value": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      }
    }
  }
}
```

## 로그 파서 구현 제안

이 포맷을 파싱하기 위한 로그 처리 파이프라인 구성:

1. **Grok 패턴**을 사용한 초기 파싱
2. **KV (Key-Value) 필터**를 사용하여 메타데이터 추출
3. **날짜 필터**를 사용하여 타임스탬프 표준화

예시 Logstash 필터 구성:

```conf
filter {
  grok {
    match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \[%{LOGLEVEL:log_level}\] \[%{DATA:service}\] \[%{DATA:transaction_id}\] \[%{DATA:host}\] %{GREEDYDATA:log_message}" }
  }
  
  kv {
    source => "log_message"
    field_split => " "
    value_split => "="
    target => "metadata"
    remove_field => "log_message"
  }
  
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
}
```

이 포맷은 구조화된 데이터와 비구조화된 데이터를 모두 포함하며, 엘라스틱서치에서 효율적으로 분석할 수 있도록 설계되었습니다.