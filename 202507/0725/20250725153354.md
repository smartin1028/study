# 판다스 대용량 데이터 처리 효율화 방법

판다스로 대용량 데이터를 처리할 때 성능 문제가 발생하는 경우, 다음과 같은 방법으로 효율성을 크게 향상시킬 수 있습니다.

## 1. 데이터 타입 최적화

```python
# 메모리 사용량 확인
df.info(memory_usage='deep')

# 범주형 데이터 변환
df['category_column'] = df['category_column'].astype('category')

# 정수형 데이터 최적화
df['int_column'] = df['int_column'].astype('int32')  # 또는 'int8', 'int16'

# 부동소수점 데이터 최적화
df['float_column'] = df['float_column'].astype('float32')
```

## 2. 청크 처리 (Chunk Processing)

```python
# 청크 단위로 데이터 읽기
chunk_size = 100000
chunks = pd.read_csv('large_file.csv', chunksize=chunk_size)

for chunk in chunks:
    # 각 청크에 대한 처리 수행
    process(chunk)
    
    # 필요시 결과 저장
    chunk.to_csv('processed_data.csv', mode='a', header=False)
```

## 3. 효율적인 데이터 읽기/쓰기

```python
# 더 빠른 파일 형식 사용
df.to_parquet('data.parquet')  # Feather, HDF5도 좋은 선택
df = pd.read_parquet('data.parquet')

# 필요한 컬럼만 읽기
cols = ['col1', 'col2']
df = pd.read_csv('large_file.csv', usecols=cols)
```

## 4. 병렬 처리

```python
# Swifter 라이브러리 사용 (자동 병렬화)
import swifter
df['new_col'] = df['column'].swifter.apply(lambda x: x*2)

# Dask 사용
import dask.dataframe as dd
ddf = dd.from_pandas(df, npartitions=4)  # 4개 파티션으로 분할
result = ddf.groupby('key').value.mean().compute()
```

## 5. 인덱싱 및 쿼리 최적화

```python
# 인덱스 설정
df = df.set_index('timestamp_column')

# 조건 필터링 최적화
# 비효율적
df[df['value'] > 100]
# 효율적
df.query('value > 100')

# isin 대신 merge 고려
valid_ids = [1, 2, 3]
# 비효율적
df[df['id'].isin(valid_ids)]
# 효율적 (대용량일 경우)
valid_df = pd.DataFrame({'id': valid_ids})
df.merge(valid_df, on='id')
```

## 6. 메모리 사용 최적화

```python
# 필요없는 컬럼 삭제
df.drop(columns=['unused_col1', 'unused_col2'], inplace=True)

# 중간 결과 삭제
del intermediate_df
import gc
gc.collect()

# 희소 데이터 처리
from scipy import sparse
sparse_matrix = sparse.csr_matrix(df.values)
```

## 7. 대체 라이브러리 사용

- **Dask**: 대용량 데이터를 여러 파티션으로 나누어 처리
- **Modin**: 판다스 API와 호환되며 멀티코어 활용
- **Vaex**: 아웃오브코어(Out-of-Core) 처리 지원
- **Polars**: Rust 기반의 고성능 데이터프레임 라이브러리

```python
# Polars 예제
import polars as pl
df = pl.read_csv('very_large_file.csv')
df.filter(pl.col('value') > 100).groupby('category').mean()
```

## 8. SQL 데이터베이스 활용

```python
# SQLite와 연동
import sqlite3
conn = sqlite3.connect(':memory:')
df.to_sql('data', conn, index=False)

# SQL 쿼리로 필터링
filtered = pd.read_sql('SELECT * FROM data WHERE value > 100', conn)
```

이러한 기법들을 상황에 맞게 조합하면 대용량 데이터 처리 시 성능을 크게 향상시킬 수 있습니다. 데이터 크기, 작업 유형, 하드웨어 사양에 따라 적절한 방법을 선택하는 것이 중요합니다.