# Python으로 로그 파일 분석 및 Pandas DataFrame 생성

다음은 로그 파일에서 메시지를 구분하여 시간, 중간 정보, 마지막 메시지 정보로 나누어 Pandas DataFrame으로 보기 편하게 처리하는 방법입니다.

## 기본 구현 코드

```python
import re
import pandas as pd

def parse_log_file(log_file_path):
    # 로그 패턴 정의 (실제 로그 형식에 맞게 수정 필요)
    log_pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) (?P<level>\w+) (?P<middle_info>\[.*?\]) (?P<message>(?:(?!\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}).)*))'
    
    log_entries = []
    current_entry = None
    
    with open(log_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # 로그 패턴 매칭 시도
            match = re.match(log_pattern, line)
            
            if match:
                # 새로운 로그 항목 시작
                if current_entry:
                    log_entries.append(current_entry)
                
                current_entry = {
                    'timestamp': match.group('timestamp'),
                    'middle_info': match.group('middle_info'),
                    'message': match.group('message').strip()
                }
            else:
                # 멀티라인 메시지 처리
                if current_entry:
                    current_entry['message'] += '\n' + line.strip()
    
    # 마지막 항목 추가
    if current_entry:
        log_entries.append(current_entry)
    
    # DataFrame 생성
    df = pd.DataFrame(log_entries)
    return df

# 사용 예시
log_df = parse_log_file('your_log_file.log')
print(log_df.head())
```

## 로그 형식에 따른 커스터마이징 방법

### 1. 다양한 로그 형식 처리

```python
def parse_custom_log(log_file_path):
    log_entries = []
    current_entry = {}
    
    with open(log_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # 예시 1: 타임스탬프가 [2023-01-01 12:00:00] 형식인 경우
            timestamp_match = re.match(r'^\[(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\]', line)
            
            # 예시 2: INFO [Middleware] 형식의 중간 정보
            info_match = re.search(r'(?P<level>\w+) \[(?P<middle_info>.+?)\]', line)
            
            if timestamp_match:
                if current_entry:  # 현재 항목이 있으면 저장
                    log_entries.append(current_entry)
                current_entry = {
                    'timestamp': timestamp_match.group('timestamp'),
                    'middle_info': info_match.group('middle_info') if info_match else '',
                    'message': line[timestamp_match.end():].strip()
                }
            else:
                if current_entry:  # 멀티라인 메시지 처리
                    current_entry['message'] += '\n' + line.strip()
    
    if current_entry:
        log_entries.append(current_entry)
    
    return pd.DataFrame(log_entries)
```

### 2. 복잡한 로그 메시지 처리

```python
def parse_multiline_log(log_file_path):
    log_entries = []
    current_entry = None
    in_multiline = False
    
    with open(log_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # 로그 시작 패턴 (예: 타임스탬프로 시작)
            if re.match(r'^\d{4}-\d{2}-\d{2}', line):
                if current_entry:
                    log_entries.append(current_entry)
                
                parts = line.split(' ', 3)  # 처음 3개의 공백으로 분리
                if len(parts) >= 4:
                    current_entry = {
                        'timestamp': ' '.join(parts[:2]),
                        'middle_info': parts[2],
                        'message': parts[3].strip()
                    }
                in_multiline = False
            else:
                if current_entry:
                    if line.strip() == '':
                        continue
                    if not in_multiline:
                        current_entry['message'] += '\n' + line.strip()
                    else:
                        current_entry['message'] += line.strip()
                    in_multiline = True
    
    if current_entry:
        log_entries.append(current_entry)
    
    return pd.DataFrame(log_entries)
```

## 결과 DataFrame 활용 예시

```python
# 로그 파일 파싱
log_df = parse_log_file('application.log')

# 1. 기본 정보 확인
print("로그 데이터 샘플:")
print(log_df.head())

print("\n로그 레벨별 개수:")
print(log_df['middle_info'].value_counts())

# 2. 특정 메시지 필터링
error_logs = log_df[log_df['middle_info'].str.contains('ERROR')]
print("\n에러 로그 개수:", len(error_logs))

# 3. 시간대별 로그 분석
log_df['timestamp'] = pd.to_datetime(log_df['timestamp'])
log_df['hour'] = log_df['timestamp'].dt.hour
print("\n시간대별 로그 발생 횟수:")
print(log_df['hour'].value_counts().sort_index())

# 4. 메시지 길이 분석
log_df['message_length'] = log_df['message'].apply(len)
print("\n메시지 길이 통계:")
print(log_df['message_length'].describe())
```

## 고급 활용 팁

1. **대용량 로그 처리**: 
   ```python
   # 청크 단위로 처리
   chunk_size = 10000
   log_chunks = pd.read_csv('large_log_file.log', 
                           chunksize=chunk_size,
                           header=None,
                           names=['raw'],
                           encoding='utf-8')
   
   processed_chunks = []
   for chunk in log_chunks:
       # 각 청크에 대해 파싱 처리
       processed = chunk['raw'].apply(parse_single_line)
       processed_chunks.append(pd.DataFrame(processed.tolist()))
   
   log_df = pd.concat(processed_chunks, ignore_index=True)
   ```

2. **정규 표현식 최적화**:
   ```python
   # 컴파일된 정규식 사용
   TIMESTAMP_PATTERN = re.compile(r'^\d{4}-\d{2}-\d{2}')
   LEVEL_PATTERN = re.compile(r'(INFO|WARN|ERROR|DEBUG)')
   
   def parse_single_line(line):
       timestamp_match = TIMESTAMP_PATTERN.match(line)
       # ... 나머지 처리
   ```

3. **로그 시각화**:
   ```python
   import matplotlib.pyplot as plt
   
   # 시간대별 로그 발생 추이
   log_df.set_index('timestamp')['middle_info'].resample('H').count().plot()
   plt.title('시간대별 로그 발생 횟수')
   plt.show()
   ```

로그의 정확한 형식에 따라 정규 표현식 패턴을 조정해야 합니다. 실제 로그 샘플을 제공하면 더 정확한 파싱 로직을 제안할 수 있습니다.