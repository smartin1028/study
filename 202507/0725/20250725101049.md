# Python Pandas를 이용한 로그 데이터 처리 방법

네, Python의 Pandas 라이브러리를 사용하면 로그 데이터를 시간과 데이터 정보로 효과적으로 표시하고 분석할 수 있습니다. 아래에 상세한 방법을 설명드리겠습니다.

## 1. 기본적인 로그 데이터 불러오기

```python
import pandas as pd

# 로그 파일 읽기 (예시: CSV 형식)
log_data = pd.read_csv('logfile.csv')

# 또는 텍스트 파일인 경우
# log_data = pd.read_table('logfile.log', sep='\s+', header=None, names=['timestamp', 'level', 'message'])
```

## 2. 시간 정보 처리

로그 데이터에서 시간 정보를 datetime 형식으로 변환:

```python
# 타임스탬프 컬럼이 있는 경우
log_data['timestamp'] = pd.to_datetime(log_data['timestamp'])

# 시간 형식이 특정 패턴인 경우 (예: '2023-08-15 14:30:22')
log_data['timestamp'] = pd.to_datetime(log_data['timestamp'], format='%Y-%m-%d %H:%M:%S')

# 인덱스로 설정하면 시간 기반 분석이 용이
log_data.set_index('timestamp', inplace=True)
```

## 3. 시간 기반 데이터 분석 예시

### 시간별 로그 수 집계
```python
# 시간별 로그 카운트
hourly_counts = log_data.resample('H').size()

# 5분 간격으로 집계
five_min_counts = log_data.resample('5T').size()
```

### 특정 시간대 데이터 필터링
```python
# 특정 날짜 데이터만 추출
daily_data = log_data['2023-08-15']

# 시간 범위 지정
time_range_data = log_data.between_time('09:00', '17:00')
```

## 4. 로그 레벨/유형별 분석

```python
# 로그 레벨별 카운트
level_counts = log_data['level'].value_counts()

# 에러 로그만 필터링
error_logs = log_data[log_data['level'] == 'ERROR']

# 특정 키워드가 포함된 로그 검색
keyword_logs = log_data[log_data['message'].str.contains('timeout', na=False)]
```

## 5. 시각화 예시

```python
import matplotlib.pyplot as plt

# 시간별 로그 트렌드 시각화
hourly_counts.plot(figsize=(12, 6), title='Logs per Hour')
plt.ylabel('Number of Logs')
plt.show()

# 로그 레벨 분포 파이 차트
level_counts.plot.pie(autopct='%1.1f%%', figsize=(8, 8))
plt.title('Log Level Distribution')
plt.show()
```

## 6. 고급 분석 기법

### 로그 패턴 분석
```python
# 로그 메시지에서 IP 주소 추출 (정규식 사용)
import re
log_data['ip_address'] = log_data['message'].str.extract(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})')

# HTTP 상태 코드 추출
log_data['status_code'] = log_data['message'].str.extract(r'HTTP\/\d\.\d"\s(\d{3})')
```

### 시계열 이상 감지
```python
# 이동 평균을 사용한 이상 감지
rolling_mean = hourly_counts.rolling(window=24).mean()
rolling_std = hourly_counts.rolling(window=24).std()

# 평균 대비 2표준편차 이상 벗어난 경우 이상으로 판단
anomalies = hourly_counts[(hourly_counts > rolling_mean + 2*rolling_std) | 
                          (hourly_counts < rolling_mean - 2*rolling_std)]
```

## 7. 멀티인덱스 사용 (여러 차원 분석)

```python
# 시간과 로그 레벨별로 그룹화
multi_index = log_data.groupby([pd.Grouper(freq='H'), 'level']).size().unstack()

# 스택된 형태로 표현
stacked = multi_index.stack().reset_index(name='count')
```

Pandas는 로그 데이터 분석을 위해 매우 강력한 도구입니다. 위 예제들을 조합하거나 필요에 맞게 수정하면 다양한 형태의 로그 분석이 가능합니다. 더 구체적인 도움이 필요하시면 로그 데이터의 샘플 형식이나 분석 목표를 알려주시면 더 자세한 안내를 드릴 수 있습니다.