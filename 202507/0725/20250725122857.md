# Python으로 로그 파일을 구분하여 H2 데이터베이스에 입력하는 방법

로그 파일을 분석하고 구분하여 H2 데이터베이스에 저장하는 과정을 단계별로 설명드리겠습니다.

## 1. 필요한 라이브러리 설치

먼저 필요한 Python 라이브러리를 설치합니다:

```bash
pip install sqlalchemy pyh2 pandas
```

## 2. H2 데이터베이스 설정

H2 데이터베이스 연결을 위한 설정입니다. 메모리 내 데이터베이스나 파일 기반 데이터베이스를 사용할 수 있습니다.

```python
from sqlalchemy import create_engine

# 메모리 내 H2 데이터베이스 연결
# engine = create_engine('h2://', echo=True)

# 파일 기반 H2 데이터베이스 연결
db_path = 'logs_db'  # 데이터베이스 파일명 (확장자 없음)
engine = create_engine(f'h2:///{db_path}', echo=True)
```

## 3. 로그 파일 파싱 및 데이터베이스 테이블 생성

로그 파일 형식에 따라 파싱 방법이 달라집니다. 예를 들어 Apache 접근 로그를 처리한다고 가정하겠습니다.

```python
from sqlalchemy import Column, Integer, String, DateTime
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime
import re

Base = declarative_base()

class LogEntry(Base):
    __tablename__ = 'log_entries'
    
    id = Column(Integer, primary_key=True)
    ip_address = Column(String(50))
    timestamp = Column(DateTime)
    request_method = Column(String(10))
    request_path = Column(String(255))
    status_code = Column(Integer)
    response_size = Column(Integer)
    user_agent = Column(String(255))
    
    def __repr__(self):
        return f"<LogEntry(ip='{self.ip_address}', path='{self.request_path}')>"

# 테이블 생성
Base.metadata.create_all(engine)
```

## 4. 로그 파일 파싱 및 데이터베이스 입력

```python
from sqlalchemy.orm import sessionmaker
import pandas as pd

# Apache 로그 형식 예시: 127.0.0.1 - - [10/Oct/2023:13:55:36 +0000] "GET /index.html HTTP/1.1" 200 2326

def parse_apache_log_line(line):
    pattern = r'^(\S+) \S+ \S+ \[([^\]]+)\] "(\S+) (\S+) \S+" (\d+) (\d+) "([^"]*)" "([^"]*)"'
    match = re.match(pattern, line)
    if not match:
        return None
    
    ip = match.group(1)
    timestamp_str = match.group(2)
    method = match.group(3)
    path = match.group(4)
    status = int(match.group(5))
    size = int(match.group(6))
    referer = match.group(7)
    user_agent = match.group(8)
    
    # 타임스탬프 파싱 (예: 10/Oct/2023:13:55:36 +0000)
    timestamp = datetime.strptime(timestamp_str, '%d/%b/%Y:%H:%M:%S %z')
    
    return {
        'ip_address': ip,
        'timestamp': timestamp,
        'request_method': method,
        'request_path': path,
        'status_code': status,
        'response_size': size,
        'user_agent': user_agent
    }

def process_log_file(file_path, batch_size=1000):
    Session = sessionmaker(bind=engine)
    session = Session()
    
    batch = []
    
    with open(file_path, 'r') as f:
        for line in f:
            parsed = parse_apache_log_line(line.strip())
            if parsed:
                log_entry = LogEntry(**parsed)
                batch.append(log_entry)
                
                if len(batch) >= batch_size:
                    session.bulk_save_objects(batch)
                    session.commit()
                    batch = []
    
    # 남은 레코드 처리
    if batch:
        session.bulk_save_objects(batch)
        session.commit()
    
    session.close()

# 로그 파일 처리
process_log_file('access.log')
```

## 5. 대용량 로그 처리 시 성능 최적화

대용량 로그 파일을 처리할 때는 다음과 같은 방법으로 성능을 향상시킬 수 있습니다:

```python
def optimized_log_processing(file_path):
    # pandas를 이용한 벌크 삽입
    data = []
    
    with open(file_path, 'r') as f:
        for line in f:
            parsed = parse_apache_log_line(line.strip())
            if parsed:
                data.append(parsed)
    
    df = pd.DataFrame(data)
    df.to_sql('log_entries', con=engine, if_exists='append', index=False)
```

## 6. 로그 유형별 구분 처리

다양한 유형의 로그를 구분하여 저장하려면:

```python
class ErrorLog(Base):
    __tablename__ = 'error_logs'
    
    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime)
    log_level = Column(String(20))
    message = Column(String(1000))
    source = Column(String(255))

def parse_error_log(line):
    # 예: [2023-10-10 14:02:15,678] ERROR: Something went wrong (module: main)
    pattern = r'^\[([^\]]+)\] (\w+): (.+) \(module: (\w+)\)'
    match = re.match(pattern, line)
    if not match:
        return None
    
    return {
        'timestamp': datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S,%f'),
        'log_level': match.group(2),
        'message': match.group(3),
        'source': match.group(4)
    }

def process_logs(log_file):
    Session = sessionmaker(bind=engine)
    session = Session()
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('['):  # 에러 로그로 가정
                parsed = parse_error_log(line)
                if parsed:
                    log_entry = ErrorLog(**parsed)
                    session.add(log_entry)
            else:  # 접근 로그로 가정
                parsed = parse_apache_log_line(line)
                if parsed:
                    log_entry = LogEntry(**parsed)
                    session.add(log_entry)
    
    session.commit()
    session.close()
```

## 7. H2 데이터베이스 관리

H2 데이터베이스 콘솔을 사용하려면:

```python
# 웹 콘솔 활성화 (개발 환경에서만 사용)
from pyh2 import Server

server = Server().start()
```

## 8. 추가 고려 사항

1. **로그 파일 모니터링**: `watchdog` 라이브러리를 사용하여 새 로그 항목을 실시간으로 처리할 수 있습니다.
2. **에러 처리**: 잘못된 로그 형식을 처리하기 위한 강력한 에러 처리 메커니즘 추가
3. **인덱싱**: 자주 쿼리되는 컬럼에 인덱스 추가
4. **분할**: 대용량 데이터의 경우 테이블 분할(partitioning) 고려

이 방법들을 조합하면 Python으로 로그 파일을 효과적으로 구분하여 H2 데이터베이스에 저장할 수 있습니다.